{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99b42f9b-b3e0-4871-9371-e8e992225893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model_vae import CVAE, cvae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b3a0215-482d-4a6a-9112-a33834bcc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.load('./data/metal.npy')[:,1:]\n",
    "c_data = np.load('./data/reaction.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "426d65c2-b20b-43b0-acdc-56358e563cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,c_train,c_test = train_test_split(x_data,c_data,test_size = 0.4,random_state = 21)\n",
    "x_val,x_test,c_val,c_test = train_test_split(x_test,c_test,test_size = 0.5,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16f76da8-c853-4115-8c65-7796bd90cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_scaler = MinMaxScaler()\n",
    "c_scaler = MinMaxScaler()\n",
    "x_train = x_scaler.fit_transform(x_train)\n",
    "c_train = c_scaler.fit_transform(c_train)\n",
    "x_val,x_test = [x_scaler.transform(x) for x in [x_val,x_test]]\n",
    "c_val,c_test = [c_scaler.transform(c) for c in [c_val,c_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5cc51fa8-d5d2-4459-9ff5-2846a6c32778",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test = [torch.tensor(x,dtype = torch.float32) for x in [x_train,x_val,x_test]]\n",
    "c_train,c_val,c_test = [torch.tensor(x,dtype = torch.float32) for x in [c_train,c_val,c_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc350c94-1605-4662-b156-ec26008c0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_data = [x_train,c_train]\n",
    "val_data =  [x_val,c_val]\n",
    "test_data =  [x_test,c_test]\n",
    "## 텐서로 바꾸기\n",
    "train_data = TensorDataset(*train_data)\n",
    "val_data = TensorDataset(*val_data)\n",
    "test_data = TensorDataset(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "350efee4-3caa-46e1-954e-a759129c2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_data,val_data,test_data]\n",
    "batch_size = 64\n",
    "train_loader,val_loader,test_loader = [DataLoader(x,batch_size=batch_size,shuffle = False) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55f44d8c-7a42-4791-b70a-132bf77da313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 9\n"
     ]
    }
   ],
   "source": [
    "x_dim = x_data.shape[1]\n",
    "c_dim = c_data.shape[1]\n",
    "print(x_dim,c_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "520dd35f-5e90-4b0a-b536-2e865ed70794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "921ede1d-a46b-486b-a53a-c8c1d18d04f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3544x32 and 64x23)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = CVAE(x_dim,c_dim,z_dim = \u001b[32m16\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# x_train.shape,c_train.shape\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m x_hat, mu,logvar = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\intern_practicing\\model_vae.py:43\u001b[39m, in \u001b[36mCVAE.forward\u001b[39m\u001b[34m(self, x, c)\u001b[39m\n\u001b[32m     41\u001b[39m mu,logvar = \u001b[38;5;28mself\u001b[39m.encoder(x,c)\n\u001b[32m     42\u001b[39m z = \u001b[38;5;28mself\u001b[39m.reparameterize(mu,logvar)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m x_hat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_hat,mu,logvar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\intern_practicing\\model_vae.py:38\u001b[39m, in \u001b[36mCVAE.decoder\u001b[39m\u001b[34m(self, z, c)\u001b[39m\n\u001b[32m     36\u001b[39m h = F.relu(\u001b[38;5;28mself\u001b[39m.dec1(h))\n\u001b[32m     37\u001b[39m h = F.relu(\u001b[38;5;28mself\u001b[39m.dec2(h))\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m x_hat = torch.sigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_hat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (3544x32 and 64x23)"
     ]
    }
   ],
   "source": [
    "model = CVAE(x_dim,c_dim,z_dim = 16)\n",
    "# x_train.shape,c_train.shape\n",
    "x_hat, mu,logvar = model(x_train,c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56112b-3728-4e09-a914-b26891d506b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=  1e-3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CVAE(x_dim,c_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99b1c0-f778-48bc-b136-417e37b87cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "train_losses = []\n",
    "val_losses =[]\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_x,batch_c in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_c = batch_c.to(device)\n",
    "        x_hat,mu,logvar = model(batch_x,batch_c)\n",
    "        loss,recon,kl = cvae_loss(x_hat,batch_x,mu,logvar,beta = 0.01)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss +=loss.item()\n",
    "    avg_train_loss = total_train_loss/len(train_loader)\n",
    "    # val\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_val_x,batch_val_c in val_loader:\n",
    "            batch_val_x = batch_val_x.to(device)\n",
    "            batch_val_c = batch_val_c.to(device)\n",
    "            v_x_hat,v_mu,v_logvar = model(batch_val_x,batch_val_c)\n",
    "            v_loss,_,_ = cvae_loss(v_x_hat,batch_val_x)\n",
    "            total_val_loss +=loss.item()\n",
    "    avg_val_loss = total_val_loss/len(val_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    if epoch %20 ==0 or epoch ==1:\n",
    "        print(f'[{epoch}/{epochs}] train_loss:{avg_train_loss:.4f}, val_loss:{avg_val_loss:.4f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_gpu)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
